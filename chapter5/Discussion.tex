\chapter{General Discussion}
\chaptermark{Discussion}

\section{Overview}

	The main aim of this thesis has been to increase our understanding of the factors that have shaped nucleotide diversity across the mammalian genome. To that end, I have focussed on understanding how processes of selection at linked sites contribute to variation in genetic diversity across the genome. Each of the projects described in the preceding three chapters have touched upon a different aspect of this. Here, I briefly summarise the main points, as regards selection at linked sites, that can be gleaned from the three projects I presented.

	In Chapter 2, I used a coalescent-based method to infer a recombination rate map from patterns of LD for \textit{M. m. castaneus}. I used this map to analyse the relationship between synonymous site diversity for protein-coding genes and local recombination rate. I found that putatively neutral diversity and recombination rate were positively correlated. Since both background selection and selective sweeps are expected to cause reductions in diversity, which are positively correlated to the rate of recombination, the relationship I found is indicative of the widespread effects of selection at linked sites. However, the positive correlation does not, on its own, carry information on the contributions of background selection and selective sweeps. The recombination map that I generated was important for the analyses performed in Chapters 3 and 4.
	
	In Chapter 3, I estimated the DFE for both harmful and advantageous mutations by analysing the unfolded site frequency spectrum (uSFS). Given the parameters obtained, I found that a combination of BGS and SSWs, could not fully explain the dips in diversity observed around functional elements in \textit{M. m. castaneus}. Using simulations, I found circumstantial evidence that selective sweeps, driven by strongly advantageous mutations, are a major contributor to the dips in putatively neutral diversity around functional elements in mice.

	In Chapter 4, I used a model that combined the effects of background selection and selective sweeps to estimate parameters of positively selected mutations that could generate the troughs in diversity observed around protein-coding exons and conserved non-coding elements (CNEs). My analyses suggested that strongly selected advantageous mutations in protein-coding exons are less frequent, but have substantially larger selection coefficients than those that occur in CNEs. Using the parameters, I found that the contribution to fitness change brought about by the substitution of advantageous mutations in protein-coding regions may somewhat outweigh that of advantageous mutations in regulatory elements.

	The main finding from the work I have carried out is that strongly advantageous mutations are chiefly responsible for the large troughs in diversity around both protein-coding exons and CNEs observed by \cite{RN122}. In Chapter 3, I showed that BGS makes a contribution to the observed troughs, but in Chapter 4 I showed that strong SSWs are required to fully explain them. I also presented evidence that, for at least two classes of functional sites in mice, there is a multimodal distribution of advantageous mutational effects. 
	 
\section{Limitations}

	There are a number of useful insights to be gleaned from the three projects described in this thesis, but the work I have done does not close the book on our understanding of selection in the house mouse genome. There are a number of assumptions that permeate this thesis, and here I give a brief description of these and the limitations they impose on the work carried out.
 
\subsection{Neutrally evolving sequences}

	 Probably the most obvious assumption I made is that that neutrality can be ascribed to certain classes of sites and regions of the genome. I made this assumption when analysing recombination rates in Chapter 2, the method I used (LDhelmet) invokes neutrality to model recombination rate variation across a chromosomal region \citep{RN213}. I also made this assumption in Chapter 3, where I assumed that the uSFSs for 4-fold and CNE-flanking sites represent neutrally evolving sequences. Although the sites may not themselves be the targets of selection, they may be influenced by selection at linked sites. There is ample evidence for selection acting on several classes of functional elements in the house mouse genome \citep{RN342, RN170} and for some classes of sites, this affects genetic diversity in surrounding genomic regions (\citealt{RN122}; Chapter 3). This will have influenced my efforts to estimate recombination rates and to estimate DFEs in Chapters 2 and 3, respectively.

\subsubsection{Recombination rate issues}

	The method used to infer recombination rate variation in Chapter 2, LDhelmet \citep{RN213}, is built upon coalescent theory, which makes the assumption that SNPs in a focal region are evolving neutrally. For a pair of SNPs, the likelihood of an observed genealogy under a particular recombination rate (in terms of $4N_er$) is calculated. By estimating the recombination rate between pair-wise combinations of SNPs, one can then build up a recombination map for a particular genomic region. Both population size change and selection at linked sites can influence linkage disequilibrium \citep{RN319}, distort genealogies away from neutral expectation \citep{RN192} and reduce the total number of segregating variants in a region \citep{RN287}. Both processes could, therefore, potentially bias and affect the resolution of recombination rate maps obtained using methods such as LDhelmet.
		
	Firstly, methods such as LDhelmet rely on the presence of SNPs, so a low number of variants in a region limits the resolution to which recombination rates can be inferred. The effects of population size change have recently been incorporated into the methodology \citep{RN381}, but this relies on accurate estimates of the demographic history of the population, which is not necessarily straightforward (\textit{see below}).
	
	Low recombination rates estimated from LD may be a caused by the effects of selection at linked sites, the true recombination rate being low, or both. If the true recombination rate were low and selection at linked sites were operating, then estimates of the recombination could potentially become severely biased. I showed in Chapter 2 that my LD-based estimates of the recombination rate are similar to pedigree-based estimates at the megabase scale, but without high resolution pedigree-based estimates of the recombination rate for \textit{M. m. castaneus} it is difficult to assess the extent of this bias at finer scales. 
	
	Finally, there is a degree of circularity in using LD-based estimates of the recombination rate for analysing selection at linked sites. For instance, in Chapter 3, we simulated positive and negative selection and set recombination rates using the LD-based estimates of $\rho$ obtained in Chapter 2. A recent selective sweep in \textit{M. m. castaneus} could result in elevated LD in a region, and thus downwardly biased estimates of $\rho = 4N_er$ in the recombination map. When simulating such regions the downwardly biased estimate of $\rho$ may have exacerbated the signal of selection at linked sites. This circularity has the effect of making the analysis in Chapters 3 conservative, but may have biased the selection parameters obtained in Chapter 4.

\subsubsection{Inferring the DFE}
	
	Estimates of the DFE for mutations that affect fitness can be obtained by contrasting the distribution of allele frequencies in a class of sites assumed to be subject to selection with that of a putatively neutrally evolving comparator. The uSFS analysis methods used in Chapters 3 and 4, DFE-alpha and polyDFE respectively, assume that the neutrally evolving reference class is interspersed among the selected sites of interest. A good example of this interspersion are synonymous and nonsynyonmous sites of protein-coding genes. However, in Chapter 3, when estimating DFEs For CNEs and UTRs, we  used neutral comparators that are tightly linked, but not interspersed among the selected site class. \cite{RN178} showed that linked putatively neutral sites can be used as a neutral reference for inferring the DFE using simulations. However, in the case of the CNEs identified by \cite{RN122}, there is evidence for the presence of functionally constrained sequences in the flanks of inferred elements. One potential consequence of selected mutations segregating in these functionally constrained sequences would be underestimation of the strength of selection by analysis of the SFS.
	
	We assumed that 4-fold sites and CNE flanking sites evolve neutrally in order to estimate DFEs from linked sites. While there is little evidence of codon-usage bias in mice, one potential source of selection on synonymous sites \citep{RN195}, there is evidence that synonymous sites within splice enhancers are conserved and are thus potentially subject to purifying selection \citep{RN369}. 
	
\subsection{Categorisation of functional elements}	
	
	Throughout Chapters 3 and 4, we have assumed that all CNEs share a single DFE. This is a reasonable starting point, but may be problematic. The CNEs analysed in this thesis were identified by \cite{RN122} using a alignment-based approach called phastCons. phastCons identifies conserved elements using an alignment of genomes, identifying individual elements using a phylogenetic hidden Markov model. The model emits discrete intervals of the genome, which are inferred to be functional on the basis of sequence conservation. In vertebrates, CNEs identified by phastCons appear to have arisen during three main time periods, apparently corresponding to the evolution of biological innovations \citep{RN353}. For example, CNEs associated with genes involved in mouse coat development appear to have arisen at a similar time to the ancestor of amniotes \citep{RN353}.
	
	CNEs identified using phastCons \citep{RN353} may play various biochemical roles, for example insulation and repression, and it seems reasonable to expect that they may be subject to different selection pressures. If this were the case, we may have incurred bias when estimating selection coefficients by treating all CNEs as a homogeneous group. Dividing the set of identified CNEs up by biochemical function would perhaps be difficult. However, with data of the kind generated by the ENCODE project, one could attempt to partition CNEs into different biochemical categories and then determine whether these categories experience different selection regimes. Ascribing biological function to phastCons elements is not necessarily straightforward, however, since the parameters used to tune the model (the expected length and coverage of conserved elements) can influence the length and number of functional elements identified \citep{RN376}. Of course, any analysis that would sub-categorise the set of CNEs would have to balance the number of categories analysed with statistical power.
	
 \subsection{Soft selective sweeps in mice}

	The work in this thesis has relied on an assumption of hard selective sweeps. Both the DFE-alpha and polyDFE analyses in Chapters 3 and 4 assume that the strength of selection acting on advantageous or deleterious mutations does not change through time. However, in a rapidly changing environment, alleles that were once neutral could become advantageous or disadvantageous in a new context. Alleles that become advantageous in this manner and subsequently fix generate soft selective sweeps \citep{RN336}. If soft sweeps were common, the assumption of hard sweeps may have influenced the outcomes of the analyses. Soft sweeps can also occur due to multiple copies of an advantageous allele arising in a population. Throughout the course of my PhD, there has been a lively debate in the literature as to the  relative importance of the soft and hard sweep models \citep{RN153, RN336}.

	If a soft sweep arises due to selection acting on standing variation, properties of the reductions in diversity are distinct from those of hard sweeps \citep{RN336}. The reason for this is that as multiple haplotypes go to fixation, more of the neutral polymorphism present before the onset of selection is preserved, causing the trough in diversity around the selected site to be shallower than for a hard sweep. If soft sweeps were the dominant mode of adaptation in mice, then the selection parameters obtained in Chapter 4 would likely be underestimates of the true values. 
	
	A crucial parameter for the probability of whether selection from standing variation results in a soft sweep or not is the frequency at which the sweeping allele was present in the population at the onset of selection, $x_0$. In the case of 0-fold nonsynonymous sites in mice, for example,  most standing variation is at low frequencies (Chapter 3). It has been argued that at the onset of selection $x_0$ will be close to $\frac{1}{2N}$, so when the allele becomes advantageous it will be present on only a very small number of haplotypes, which would likely lead to a hard sweep \citep{RN153}. In contrast, \cite{RN336} showed that if the distribution of $x_0$ (i.e. the site frequency spectrum) is taken into account the probability of a soft sweep from standing variation is far higher than when assuming a single fixed value. The reason for this is that the adaptation acting on higher frequency standing variants are far more likely to cause soft sweeps. Since site frequency spectra for different classes of sites differ, the probabilities of soft sweeps occurring in different classes of functional elements may differ. 

	\cite{RN338} used a machine learning approach to classify regions of the human genome as either having experienced or having been linked to a hard or soft selective sweep. They used a method, S/HIC, which uses `random forest' classification methods \citep{RN337}. The basic protocol is as follows: Data are simulated under a specific model, e.g. neutrality, soft sweeps or hard sweeps, and summary statistics are calculated. Many simulations are performed varying a range of parameters, such as recombination rate and strength of selection. From a randomly sampled subset of these simulations, summary statistics are extracted and used to generate a decision tree that, when presented with a new set of summary statistics, classifies the input dataset as having been generated under a particular model (in this case neutral, hard or soft sweep or linked to a hard or soft sweep). This process is repeated with many randomly generated trees (populating the `random forest'). The model that obtains the most classifications (or `votes') from the large set of random trees provides insight into the underlying process that generated the data. 
	
	\cite{RN338} applied S/HIC to human polymorphism data from the 1000 Genomes project \citep{RN272} and found that soft selective sweeps are far more common than hard sweeps. One of the useful properties of the `random forest` methods employed by \cite{RN338} is that they allow one to rank the most influential summary statistics. In the paper describing S/HIC, \cite{RN337} analysed human chromosome 18 and reported that H2/H1, a statistic that summarises the distribution of haplotype frequencies in a sample \citep{RN208}, was highly influential for their classifications. This is of note, because allelic gene conversion, which was not included in the simulations they used to build their machine learning classifiers, can cause the haplotype distribution under hard sweeps to resemble that of soft sweeps \citep{RN366}. It remains to be seen whether the findings of \cite{RN338} are robust to the effects of gene conversion.
	
	If soft sweeps are the dominant mode of adaptation in humans as the analyses of \cite{RN338} suggest, then it seems likely that it would also be so for mice. On the basis of within-species polymorphism, humans are estimated to have a $\theta = 4N_e\mu$ of around 0.001 \citep{RN398}. The probability of soft selective sweeps occurring from either standing genetic variation or recurrent mutations are proportional to $\theta$ \citep{RN336}, so wild mice, which have an estimated $\theta$ of around 1\%, should thus be more predisposed to soft sweeps than humans. If there were evidence for frequent soft sweeps in mice, which was robust to the effects of gene conversion, then the analysis methods used throughout this thesis would have to be scrutinised under a model of adaptation from standing variation or from multiple mutations.
 
  \subsection{Dominance and Haldane's sieve}
 
	In 1927 Haldane demonstrated that newly introduced recessive beneficial mutations are far more likely to be lost by chance than dominant mutations with the same selective advantage (Haldane 1927). This effect, which has become to be known as Haldane's sieve, thus predicts that most beneficial mutations that become fixed are dominant (assuming that an equal number of dominant and recessive mutations arise). If adaptation proceeds from standing variation, Haldane's sieve may not be relevant, because recessive and dominant alleles have similar probabilities of fixation when they are at high frequencies \citep{RN395}. My analyses of the uSFS and of patterns of genetic diversity in Chapters 3 and 4 relied on models which assume that all new mutations are additive (or semi-dominant) in their effects. In the case of the analyses in Chapter 4, as long as mutations are neither fully recessive nor fully dominant (0 $<$ \textit{h} $<$ 1), the troughs in diversity resulting from mutations with the compound parameter \textit{2hs} (the dominance coefficient $h$ and a selection coefficient $s$) are similar \citep{RN396}. Because of this, if new mutations are neither fully recessive nor fully dominant, the selection coefficients estimated from the patterns of diversity they leave behind should be directly proportional to the true values. It is somewhat unclear, however, how varying dominance coefficients would influence SFS-based analyses.
	
\subsection{The interaction between natural selection and demographic history}

	This thesis has focussed on the effects of background selection and selective sweeps, and has assumed, except where explicitly modelled, that the demographic history of \textit{M. m. castaneus} has not influenced the analyses. This may potentially bias the results presented in Chapters 3 and 4, because the effects of BGS can become amplified under population size change \citep{RN397}. In Chapter 3, we inferred that \textit{M. m. castaneus} has recently undergone a dramatic population expansion, a result obtained from two quasi-independent classes of putatively neutral sites (4-fold degenerate synonymous sites and CNE-flanking sites). It is tempting to interpret these results in light of recent human history: Mice are commensal to humans so their population numbers have likely exploded in the recent past. However, as we also showed in Chapter 3, selection at linked sites can cause one to infer a population expansion even there is not one. \textit{M. m. castaneus} may have undergone a rapid population expansion in the recent past, but it is likely that the demographic parameters we inferred are highly influenced by selection at linked sites.
	
	Across the \textit{M. m. castaneus} genome, there is a strongly negative Tajima's $D$ of around -0.5, consistent with both widespread selection at linked sites and a recent population expansion. In Chapter 3, we showed that selection at linked sites (as generated by the DFEs we inferred from the mouse population data we analysed) does not result Tajima's $D$ values as negative as those observed. Even when I modelled relatively strong selection ($\gamma_a = 800$), SSWs resulted in a localised trough in Tajima's $D$ around protein-coding exons, but this recovered almost to 0 in surrounding regions (Figure C.8). This suggests that the seemingly genome-wide negative Tajima's $D$ is not solely explained by the effects of selection at linked sites, but of course there may be a combination of selection parameters that generate the observed values. Additionally, it is possible that relatively recent demographic processes have erased the signal of selection at linked sites across the genome. To fully investigate this possibility, however, estimates of the demographic history for mice, unbiased by the effects of selection at linked sites are required. There are number of strategies that could be employed to obtain these.

\section{Moving Forward}	

	There are many possible directions that could be taken with to further our understanding of the factors that shape patterns of genetic diversity across the mouse genome. As the final part of this thesis, I will describe several possible areas for further study.
	
\subsection{Robust demographic models}

	The majority of demographic models assume neutrally evolving sites, so it is desirable to parametrise them from regions of the genome that are free from the effects of selection at linked sites. One could use regions of the genome far from functional elements (both coding and non-coding), because these are expected to be the most free from the effects of selection at linked sites, especially if they are in highly recombining regions. One could go a step further and fit a model of selection at linked sites to genome-wide polymorphism data (e.g. \citealt{RN274}) and identify regions that only experience small effects of selection. However, such methods rely on a perfect knowledge of the locations of functional elements in the genome. Since the methods used to identify conserved elements may fail to detect rapidly evolving sequences and weakly selected regions, there is the possibility that BGS and SSWs influence genetic variation even when there are no annotations present. An alternative strategy would be to use methods such as the machine learning method S/HIC, which uses a machine learning classifier that can discriminate between neutrally evolving sequences and sequences influenced by selection at linked sites by `learning` the properties of neutrally evolving sequences on the basis of summary statistics. Parametrising demographic models from the neutrally evolving sequences identified using the above described methods may give estimates of the demographic history that are least affected by selection at linked sites. 

\subsection{Making better use of the available data}

	A substantial hurdle to population genomic research is in making use of all the available data. For example, in Chapters 3 and 4 I have analysed either the site frequency spectrum or nucleotide diversity. These are just two data summaries that can be conveniently analysed in  population genetic models, but there are others. As I demonstrated in Chapter 4, the SFS is a useful summary of the data that can be used to estimate the distribution of fitness effects for harmful variants, but the uSFS can be uninformative for estimating the parameters of strongly selected advantageous mutations, particularly if they are rare. In such cases, patterns of genetic diversity are more informative. Ideally, one would make use of information in both the uSFS for potentially selected sites, whilst simultaneously modelling the reductions in neutral diversity caused by selection at those sites. There is information about the effects of selection in both linkage disequilibrium and population haplotype structure, but these may be very difficult to incorporate into an analytical expression along with the SFS and diversity at linked sites. One possibility for using as much of the available data as possible would be to perform approximate Bayesian computation (ABC) or machine learning with forward-in-time population genetic simulations.

	The basic idea is as follows: Simulate data under a model, sampling the parameters of interest from  plausible ranges, and compare summary statistics from your dataset to those obtained by simulation \citep{RN356}. The parameter sets that generated summary statistics most resembling those in the data give an estimate of the underlying parameters. This is the basic idea behind ABC and machine learning approaches so far developed in population genomics, although ML methods have the benefit of allowing the user to assess the importance of particular parameters \citep{RN377}. In the context of inferring the dDFE and positive selection parameters, one could simulate a chromosome or chromosomal regions with the same structure as the species of interest (like I did in Chapter 3). Many thousands of different combinations of DFE parameters could be simulated, and from these data, one extract summary statistics for the site frequency spectrum, linkage disequilibrium and haplotype structure within and in the regions surrounding, several classes of functional elements. The biggest barrier to applying an analysis such as this is the computational demands of the many simulations required.

	The simulations used in this thesis were performed with SLiM v1.8, a program which was, at the time of its release, among the most computationally efficient forward-in-time simulators available \citep{RN148}. Forward simulators have historically been much slower than coalescent simulators because the evolution of whole chromosomes is typically tracked. In the original SLiM publication, \cite{RN148} described how by tracking just the mutations, simulations of purifying selection acting on a whole human chromosome (100Mbp long; $10^4$ diploid individuals; for $10^5$ generations) took just four days. As impressive as that is, it is not feasible to perform ABC or ML using such simulations. In the four years since starting my PhD a number of increasingly efficient forward-in-time simulators have been developed \citep{RN361, RN362, RN360}, but even with these it would be difficult to perform ABC or ML as described. However, very recent advances in computational efficiency of forward-in-time simulators \citep{RN359} may bring approaches of the kind outlined above within reach.

\subsection{Of mice and men (and fruit flies)}

	The vast majority of the literature cited in this thesis has involved mice, humans or \textit{D. melanogaster}. Being three of the most well studied organisms in biology means that the genomic resources available for these three species are excellent. However, it is not obvious how generalisable findings in these groups are. There are substantial barriers to performing population genomic studies in non-model organisms, for example in obtaining a reference genome and in obtaining functional annotations of the genome \citep{RN382}. However, analysing close relatives of model organisms is a way to give evolutionary studies a broader focus. A number of studies in great apes provide a template for comparisons between closely related species. For instance, across great ape species \cite{RN221} showed that there is a negative correlation between recombination rate similarity and nucleotide divergence and \cite{RN365} concluded that strong sweeps are largely responsible for reductions in diversity near genes. In both these studies, reference genomes and annotations obtained for the model organisms were used when analysing sister species. Similar comparative studies could be performed in murid rodents, using available datasets; population samples of the three principle \textit{M. musculus} sub-species and \textit{Mus spretus} \citep{RN383} and the brown rat \citep{RN327} are publicly available. 

	One of the intriguing findings from Chapter 4 was that adaptation in protein-coding regions may be driven by mutations that are, on average, far stronger than those that occur in regulatory regions. Performing analyses using the rodent datasets described above could further our understanding of whether adaptation in protein-coding regions contributing more to fitness change than regulatory regions is a general feature of mammalian evolution or specific to \textit{M. m. castaneus}.